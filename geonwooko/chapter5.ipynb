{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d330f085",
   "metadata": {},
   "source": [
    "# 5. PROGRAMMING WITH RDDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45c8935",
   "metadata": {},
   "source": [
    "## RDD란?\n",
    "RDD는 Resilient Distributed Dataset의 줄임말로, objects sets의 immutable distributed collection이다.\n",
    "각 RDD는 여러개의 파티션으로 나누어지고, 각 파티션은 클러스터의 여러 노드에 의해 분배된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77d8220",
   "metadata": {},
   "source": [
    "## 5.1 Create RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356fc934",
   "metadata": {},
   "source": [
    "방법 1: parallelize 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "057c1c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/08/10 17:42:40 WARN Utils: Your hostname, jamess-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 172.29.112.67 instead (on interface en0)\n",
      "25/08/10 17:42:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/08/10 17:42:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+-----+\n",
      "|col1|col2|col3| col4|\n",
      "+----+----+----+-----+\n",
      "|   1|   2|   3|a b c|\n",
      "|   4|   5|   6|d e f|\n",
      "|   7|   8|   9|g h i|\n",
      "+----+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"chapter5\").getOrCreate()\n",
    "\n",
    "df = spark.sparkContext.parallelize(\n",
    "    [(1, 2, 3, \"a b c\"), (4, 5, 6, \"d e f\"), (7, 8, 9, \"g h i\")]\n",
    ").toDF([\"col1\", \"col2\", \"col3\", \"col4\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b641b9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42da7aa",
   "metadata": {},
   "source": [
    "2. createDataFrame() 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7548a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"chapter5\").getOrCreate()\n",
    "\n",
    "Employee = spark.createDataFrame(\n",
    "    [\n",
    "        (1, \"James\", \"Smith\", \"1991-04-01\", \"M\", 3000),\n",
    "        (2, \"Michael\", \"Rose\", \"2000-05-19\", \"M\", 4000),\n",
    "        (3, \"Robert\", \"Williams\", \"1978-09-05\", \"M\", 4000),\n",
    "    ],\n",
    "    [\"id\", \"firstname\", \"lastname\", \"dob\", \"gender\", \"salary\"],\n",
    ")\n",
    "\n",
    "Employee.show()\n",
    "\n",
    "Employee.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a79b37",
   "metadata": {},
   "source": [
    "3. read / load를 이용해 파일을 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61564f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로컬에 있는 데이터 파일을 읽어오기\n",
    "df = (\n",
    "    spark.read.format(\"com.databricks.spark.csv\")\n",
    "    .options(header=\"true\", inferSchema=\"true\")\n",
    "    .load(\"./LearningApacheSpark/chapter5/data.csv\")\n",
    ")\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1729fca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9b39f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터베이스 있는 파일 읽어오기\n",
    "\n",
    "# jdbc(java database connectivity)를 이용\n",
    "url = f\"jdbc:mysql://localhost:3306/mydb\"\n",
    "\n",
    "df = (\n",
    "    spark.read.format(\"jdbc\")\n",
    "    .options(\n",
    "        url=url,\n",
    "        driver=\"com.mysql.cj.jdbc.Driver\",\n",
    "        dbtable=\"users\",\n",
    "        user=\"root\",\n",
    "        password=\"password\",\n",
    "    )\n",
    "    .load()\n",
    ")\n",
    "\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b4b001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDFS 파일 읽어오기\n",
    "\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "sc = SparkContext(\"local\", \"example\")\n",
    "hc = HiveContext(sc)\n",
    "tf1 = sc.textFile(\"hdfs://localhost:9000/user/chapter5/data.csv\")\n",
    "\n",
    "# intg_cme_w 데이터베이스 사용\n",
    "hc.sql(\"use intg_cme_w\")\n",
    "\n",
    "# 해당 데이터베이스에서 쿼리\n",
    "spf = hc.sql(\"select * from spf\")\n",
    "spf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4e14c6",
   "metadata": {},
   "source": [
    "## 5.2 Spark Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c959803",
   "metadata": {},
   "source": [
    "스파크에서 operations은 크게 두가지 종류 (transformations & actions)로 구분된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2e3a28",
   "metadata": {},
   "source": [
    "### 5.2.1 Spark Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce213322",
   "metadata": {},
   "source": [
    "transformation은 이전의 RDD를 변형해 새로운 RDD를 생성한다(map에 해당). 아래 사진의 함수들이 존재한다.\n",
    "![](2025-08-10-17-07-42.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e47d53",
   "metadata": {},
   "source": [
    "### 5.2.2 Spark Actions\n",
    "\n",
    "action은 반대로 RDD를 aggregation하여 결과물을 얻어낸다. (reduce에 해당)\n",
    "![](2025-08-10-17-09-24.png)\n",
    "![](2025-08-10-17-09-30.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9808f8d",
   "metadata": {},
   "source": [
    "### 5.3.2 Load DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9412f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래처럼 로컬 파일을 읽어올 수 있다\n",
    "ds = spark.read.csv(\n",
    "    path=\"Advertising.csv\",\n",
    "    # sep=',',\n",
    "    # encoding='UTF-8',\n",
    "    # comment=None,\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c44f86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json 파일 읽어오기\n",
    "!wget https://api.luftdaten.info/static/v1/data.json\n",
    "ds = spark.read.json(\"data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75465403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 head row\n",
    "ds.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b6ed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data types of each column\n",
    "ds.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743bf222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns\n",
    "ds.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013620b3",
   "metadata": {},
   "source": [
    "### Fill  Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5008fd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = [[\"male\", 1, None], [\"female\", 2, 3], [\"male\", 3, 4]]\n",
    "ds = spark.createDataFrame(my_list, [\"gender\", \"age\", \"salary\"])\n",
    "ds.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadd0f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.fillna(-99).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdd755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace values\n",
    "# caution: Mixed type replacements are not supported\n",
    "ds.na.replace([\"male\", \"female\"], [\"M\", \"F\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb321d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "ds.toDF(\"a\", \"b\", \"c\").show()\n",
    "\n",
    "ds.withColumnRenamed(\"gender\", \"a\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484ec66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "ds.drop(\"gender\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab72423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter\n",
    "ds.filter(ds.gender == \"male\").show()\n",
    "\n",
    "# or\n",
    "ds[ds.gender == \"male\"].show()\n",
    "\n",
    "# multiple filter\n",
    "ds[(ds.gender == \"male\") & (ds.age > 2)].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95fadd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# make new column\n",
    "ds.withColumn(\n",
    "    \"age_norm\",\n",
    "    ds.age / ds.groupBy().agg(F.sum(\"age\")).collect()[0][0],  # sum of ages\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2ad6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complicated function\n",
    "ds.withColumn(\n",
    "    \"cond\", F.when(ds.gender == \"male\", 1).otherwise(0)\n",
    ").show()  # male: 1, female: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477d095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame 복사 방법들\n",
    "new_ds = ds.toDF(*[\"a\", \"age\", \"c\"])  # 모든 컬럼 선택으로 복사\n",
    "\n",
    "# right (left, inner, full도 가능) join\n",
    "ds.join(new_ds, on=\"age\", how=\"right\").orderBy(\"A\", ascending=True).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808b52d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat two columns\n",
    "ds.withColumn(\"concat\", F.concat(ds.gender, ds.age)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d842187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby\n",
    "# null인 값은 무시된다.\n",
    "ds.groupBy(\"gender\").agg({\"age\": \"sum\", \"salary\": \"avg\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6c83ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot\n",
    "ds.groupBy(\"gender\").pivot(\"age\").agg(F.sum(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7b1a9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/james/Study/pyspark/.venv/lib/python3.9/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/Users/james/Study/pyspark/.venv/lib/python3.9/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  A|  B|  C|\n",
      "+---+---+---+\n",
      "|  a|  m|  1|\n",
      "|  b|  m|  2|\n",
      "|  c|  n|  3|\n",
      "|  d|  n|  6|\n",
      "+---+---+---+\n",
      "\n",
      "+---+---+---+----+\n",
      "|  A|  B|  C|rank|\n",
      "+---+---+---+----+\n",
      "|  a|  m|  1|   1|\n",
      "|  b|  m|  2|   2|\n",
      "|  c|  n|  3|   1|\n",
      "|  d|  n|  6|   2|\n",
      "+---+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# window\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "\n",
    "d = {\"A\": [\"a\", \"b\", \"c\", \"d\"], \"B\": [\"m\", \"m\", \"n\", \"n\"], \"C\": [1, 2, 3, 6]}\n",
    "df = pd.DataFrame(d)\n",
    "ds = spark.createDataFrame(df)\n",
    "\n",
    "ds.show()\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = Window.partitionBy(\"B\").orderBy(\"A\")\n",
    "\n",
    "ds.withColumn(\"rank\", F.rank().over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db83521e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b6d84e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
