# Spark SQL
- 정형 데이터(structured data)를 SQL 쿼리나 데이터프레임(DataFrame)을 이용해 쉽게 처리할 수 있게 해주는 모듈
- DB 의 테이블 뿐만아니라 데이터프레임에도 sql 을 실행할 수 있음
- CREATE 문을 사용해서 뷰를 만들어서 사용하거나 SELECT * FROM /path/test.json 과 같이 해당 파일로 직접 접근할 수도 있음

## DataFrame API VS Spark SQL
### DataFrame API
- df.where(...), df.select()처럼 DataFrame 객체의 메서드를 체인 형태로 호출하여 데이터를 가공하는 방식

``` python
path = '/Users/younghun/Desktop/gitrepo/data/titanic/train.csv'

spark_df = spark.read.csv(path, header=True, inferSchema=True)
spark_df.limit(num=5).show()
```
``` python
# df 라는 데이터프레임이 있다고 가정
df = ...

flights_df = df \
    .where(col("flight_duration_hours") > 12) \
    .select("DEST_COUNTRY_NAME", "count")

long_flights_df.show()
```
### Spark SQL
- spark.sql("SELECT ...") 와 같이 쿼리문을 문자열 형태로 직접 실행하는 방식

``` python
# df 라는 데이터프레임이 있다고 가정
df = ...

# df 를 뷰로 등록
df.createOrReplaceTempView("flights_view")

# SQL 쿼리문을 문자열로 실행
sql = 
"""
    SELECT DEST_COUNTRY_NAME, count
      FROM flights_view 
     WHERE flight_duration_hours > 12
"""
long_flights_df = spark.sql(sql)

long_flights_df.show()
```

#### 참조
- https://techblog-history-younghunjo1.tistory.com/498#google_vignette

## Spark SQL CLI(Command-Line Interface)
- pyspark 가 ETL, ML 등 개발과 가깝다면 SQL CLI 는 주로 데이터를 조회할 때 사용
- 실시간 스트리밍 환경에는 부적합하다

### 사용 예시
``` bash
# 2015-summary.json 에 쿼리한 결과를 테이블 형태로 저장
CREATE TABLE flights_summary
USING parquet
LOCATION '/user/hive/warehouse/flights_summary'
AS
SELECT
    DEST_COUNTRY_NAME,
    SUM(count) AS total_count
FROM json.`/data/flight-data/json/2015-summary.json` -- .json 경로를 직접 지정 가능
WHERE DEST_COUNTRY_NAME LIKE 'S%' 
GROUP BY DEST_COUNTRY_NAME
HAVING  total_count > 10
;

# 이후 해당 테이블에 직접 쿼리 가능
SELECT * 
  FROM flights_summary
;

```

### 참조
- https://wave35.tistory.com/122#10%EC%9E%A5%C2%A0Spark%C2%A0SQL-1
- https://ingu627.github.io/spark/spark_db15/
