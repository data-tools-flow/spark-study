# Apache Spark 란?
- 대규모 데이터 처리용 통합 분석 엔진으로 클러스터화된 컴퓨터군에서 빅데이터 작업을 분산/병렬로 처리하기 위한 오픈 소스 프레임워크

## 분산/병렬 처리?
- 병렬 처리: 하나의 컴퓨터 안에서 작업을 분할하여 처리
- 분산 처리: 여러 대의 컴퓨터에서 작업을 분할하여 처리

#### !!! 대용량 데이터를 여러 개의 조각으로 나눠서 처리한다
- 분산 처리의 경우 여러 대의 서버에 나눠서 동시에 처리 후 다시 합치는 방식
- 병렬 처리의 경우 여러 개의 CPU 코어를 모두 활용해서 데이터를 처리

### 스파크 구성 요소
1. 드라이퍼 프로그램(Driver Program)
- 사용자가 작성한 스파크 코드를 보고 전체 실행 계획을 세움
- 클러스터 관리자에게 필요한 자원을 요청함

2. 클러스터 관리자(Cluster Manager)
- 마스터 노드에서 실행됨
- 현재 자원 사황을 파악하고 요청에 맞게 분배(리소스 할당)

3. 실행기(Executor)
- 워커 노드에서 실행됨
- 실제 작업의 실행 단위


#### 참조
- https://blog.naver.com/ryanyangsa/223673300055

<img src="/kimtaewook/images/빅데이터 처리 기술의 발전.png" width="1000"/>   

**1. ~2010년대**
- 데이터 처리 요구사항이 막 생겨나기 시작함
- 하둡(Hadoop)은 처음으로 대용량 데이터를 여러 컴퓨터에 나누어 저장(HDFS)하고 처리(MapReduce) 했음
- 디스크 기반의 처리 방식:작업을 수행할 때마다 중간 결과를 **하드 디스크(HDFS)**에 읽고 쓰는 과정을 반복
    - 디스크 입출력(I/O) 병목 현상이 발생하여 속도가 느림

**2. ~2020년대**
- 2010년 이후 데이터 처리 요구사항이 급격하게 증가함
- 스파크가 등장(CPU 기반)
- 인메모리 방식:중간 처리 결과를 하드 디스크가 아닌 메모리(RAM) 에 저장   
    - 디스크 I/O(입출력)에 비해 속도가 월등히 빠름, 특히 동일한 데이터를 반복적으로 사용하는 머신러닝과 같은 작업에서 엄청난 성능 향상
- 하지만 인공지능(AI)과 머신러닝의 발전으로 스파크의 CPU 처리 능력이 한계에 다다름

**3. 2020년 이후**
- 더욱 높아지는 데이터 처리 요구사항에 아파치 스파크3.0 부터는 GPU를 활용하기 시작
- GPU는 수많은 코어를 이용해 대규모 연산을 동시에 처리하는 병렬 처리에 매우 뛰어남

## 왜 꼭 스파크여야 하나?
- 스파크의 기능 중 실시간 데이터 처리의 경우 아파치 플링크(Apach flink) 가 더 나은 선택이 될 수도 있지만 대용량 데이터를 빠르게 분석/처리하는 분야에서는 아직까지 스파크를 대체할만한 수단이 없다고 생각함

### 참조
- https://cloud.google.com/learn/what-is-apache-spark?hl=ko
- https://www.nvidia.com/ko-kr/glossary/apache-spark/